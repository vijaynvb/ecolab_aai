{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tjZg_lD5FCQ"
      },
      "source": [
        "# **Models** - The interface to the AI brains\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "Design and implement various AI models using the LangChain and OpenAI libraries to understand their functionalities and applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FpcUiTJV5JMy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install --upgrade langchain langchain_community langchain-openai\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yc8ym6AS5ULU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('OPENAI_API_VERSION')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-zBN24Z7gjo"
      },
      "source": [
        "## **Chat Model**\n",
        "A model that takes a series of messages and returns a message output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JlnD-App6YXd"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "\n",
        "\n",
        "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
        "chat = AzureChatOpenAI(deployment_name=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ukOg4hnU7j_v",
        "outputId": "f33e125f-c691-43b2-fa50-9beec70e8961"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/rm/gvl01dcd5dg9dxwkn8zgg9v40000gn/T/ipykernel_64777/3040646992.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chat(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Well, you could try flapping your arms really hard, but I hear flights are faster and less awkward-looking!'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
        "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
        "    ]\n",
        ")\n",
        "response.content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8pUU30G8Yy_"
      },
      "source": [
        "## **Function Calling Models**\n",
        "\n",
        "Function calling models are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
        "\n",
        "This comes in handy when you're making an API call to an external service or doing extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add get current weather function\n",
        "def get_current_weather(location: str ) -> str:\n",
        "    # This is a mock function. In a real application, you would call a weather API.\n",
        "    return f\"The current weather in {location} is sunny with a high of 75°F.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCWOYzrH8Its",
        "outputId": "76c4a9e5-b86c-4fbc-edd8-26fe44f83cfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Boston\"}', 'name': 'get_current_weather'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 89, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-C6WGIvTEPeSotqiugmwKmKSFxVuDc', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'function_call', 'logprobs': None, 'content_filter_results': {}}, id='run--535a9aa6-5268-4453-8b99-af9c2289fd4c-0', usage_metadata={'input_tokens': 89, 'output_tokens': 16, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = AzureChatOpenAI(deployment_name='gpt-4o')\n",
        "\n",
        "output = chat(messages=\n",
        "     [\n",
        "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
        "         HumanMessage(content=\"What’s the weather like in Boston right now?\")\n",
        "     ],\n",
        "     functions=[{\n",
        "         \"name\": \"get_current_weather\",\n",
        "         \"description\": \"Get the current weather in a given location\",\n",
        "         \"parameters\": {\n",
        "             \"type\": \"object\",\n",
        "             \"properties\": {\n",
        "                 \"location\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                 },\n",
        "                 \"unit\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "                 }\n",
        "             },\n",
        "             \"required\": [\"location\"]\n",
        "         }\n",
        "     }\n",
        "     ]\n",
        ")\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU9n0gKp-pU"
      },
      "source": [
        "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUJpftZ83kg"
      },
      "source": [
        "## **Text Embedding Model**\n",
        "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk8r4-ru8ov6"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "# Initialize Azure OpenAI embeddings\n",
        "embeddings = AzureOpenAIEmbeddings(azure_deployment=\"text-embedding-ada-002\", api_key=\"\", azure_endpoint=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RLEMmaN887_P"
      },
      "outputs": [],
      "source": [
        "text = \"Hi! It's time for the beach\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RujU35zb9Dfc",
        "outputId": "801ce5ae-80dd-4471-a77e-7c763f550741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a sample: [-0.00022214034106582403, -0.0031126115936785936, -0.0010768607025966048, -0.019214099273085594, -0.015184946358203888]...\n",
            "Your embedding is length 1536\n"
          ]
        }
      ],
      "source": [
        "text_embedding = embeddings.embed_query(text)\n",
        "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
        "print (f\"Your embedding is length {len(text_embedding)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B40kqdxBElD"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Create a simple project using the different AI models discussed (language model, chat model, function-calling model, and text embedding model) to understand their functionalities and applications.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* **Set Up**: Install the necessary libraries and set up your OpenAI API key.\n",
        "* **Language Model**: Implement a language model to generate text based on a given prompt.\n",
        "* **Chat Model**: Implement a chat model to simulate a conversation.\n",
        "* **Function Calling Model**: Implement a function-calling model to generate structured data outputs.\n",
        "* **Text Embedding Model**: Implement a text embedding model to convert text into a vector representation.\n",
        "* **Demonstration**: Provide examples for each model to demonstrate their usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgRbcjcZBhWR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
