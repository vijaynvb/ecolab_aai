{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dt9_nCtGCJku",
      "metadata": {
        "id": "dt9_nCtGCJku"
      },
      "source": [
        "# **Memory**\n",
        "\n",
        "The memory allows a Large Language Model (LLM) to remember previous interactions with the user. By default, LLMs are stateless â€” meaning each incoming query is processed independently of other interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224cfbf2",
      "metadata": {
        "id": "224cfbf2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade langchain langchain_community langchain-openai\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab75d1a0",
      "metadata": {
        "id": "ab75d1a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('OPENAI_API_VERSION')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eAiHVbID3D8d",
      "metadata": {
        "id": "eAiHVbID3D8d"
      },
      "source": [
        "# **ConversationBufferMemory**\n",
        "\n",
        "ConversationBufferMemory usage is straightforward. It simply keeps the entire conversation in the buffer memory up to the allowed max limit (e.g. 4096 for gpt-3.5-turbo, 8192 for gpt-4). The main downside, however, is the cost. Each request is sending the aggregation to the API. Since the charge is based on token usages, the cost can quickly add up, especially if you are sending requests to an AI platform. Additionally, there can be added latency given the sheer size of the text thatâ€™s being passed back and forth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "060ae786",
      "metadata": {
        "id": "060ae786"
      },
      "outputs": [],
      "source": [
        "# ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "94d72e64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94d72e64",
        "outputId": "c2d81ca0-2231-4b91-fe88-ef244e933ff8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4-158765928.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-4-158765928.py:7: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o\",\n",
        "    temperature = 0.5\n",
        ")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose= True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39f30637",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "39f30637",
        "outputId": "ee5c66f9-7b61-4d6b-b8d5-3ba7f61e8ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Andrew\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello, Andrew! It's great to meet you. My name doesn't have the same ring to it as yoursâ€”I'm just an AIâ€”but I'm here to chat, answer questions, and share all sorts of interesting information. How can I help you today?\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7747d73f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "collapsed": true,
        "id": "7747d73f",
        "outputId": "4dbac2fc-14bc-4331-9ffa-3a996bd96179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello, Andrew! It's great to meet you. My name doesn't have the same ring to it as yoursâ€”I'm just an AIâ€”but I'm here to chat, answer questions, and share all sorts of interesting information. How can I help you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Ah, the classic math question! 1 + 1 equals 2. It's one of the simplest and most fundamental equations in arithmetic, but it's also a great starting point for understanding how numbers work. Let me know if you'd like to dive into anything more complex or interesting!\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0b4fada3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "collapsed": true,
        "id": "0b4fada3",
        "outputId": "1280eddd-a1c4-44a6-98f8-de07df6fc6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello, Andrew! It's great to meet you. My name doesn't have the same ring to it as yoursâ€”I'm just an AIâ€”but I'm here to chat, answer questions, and share all sorts of interesting information. How can I help you today?\n",
            "Human: What is 1+1?\n",
            "AI: Ah, the classic math question! 1 + 1 equals 2. It's one of the simplest and most fundamental equations in arithmetic, but it's also a great starting point for understanding how numbers work. Let me know if you'd like to dive into anything more complex or interesting!\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your name is Andrew! You just introduced yourself a moment ago. Iâ€™ve got a pretty good memory for conversations, so feel free to ask me about anything weâ€™ve talked about so far. ðŸ˜Š'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ae7024a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae7024a8",
        "outputId": "150ccb5e-110e-4d3a-8526-29d97ce9cde0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi, my name is Andrew\n",
            "AI: Hello, Andrew! It's great to meet you. My name doesn't have the same ring to it as yoursâ€”I'm just an AIâ€”but I'm here to chat, answer questions, and share all sorts of interesting information. How can I help you today?\n",
            "Human: What is 1+1?\n",
            "AI: Ah, the classic math question! 1 + 1 equals 2. It's one of the simplest and most fundamental equations in arithmetic, but it's also a great starting point for understanding how numbers work. Let me know if you'd like to dive into anything more complex or interesting!\n",
            "Human: What is my name?\n",
            "AI: Your name is Andrew! You just introduced yourself a moment ago. Iâ€™ve got a pretty good memory for conversations, so feel free to ask me about anything weâ€™ve talked about so far. ðŸ˜Š\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6bbabbc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbabbc7",
        "outputId": "757feaab-ecdc-4462-fb2d-1dcc2a8e8d33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello, Andrew! It's great to meet you. My name doesn't have the same ring to it as yoursâ€”I'm just an AIâ€”but I'm here to chat, answer questions, and share all sorts of interesting information. How can I help you today?\\nHuman: What is 1+1?\\nAI: Ah, the classic math question! 1 + 1 equals 2. It's one of the simplest and most fundamental equations in arithmetic, but it's also a great starting point for understanding how numbers work. Let me know if you'd like to dive into anything more complex or interesting!\\nHuman: What is my name?\\nAI: Your name is Andrew! You just introduced yourself a moment ago. Iâ€™ve got a pretty good memory for conversations, so feel free to ask me about anything weâ€™ve talked about so far. ðŸ˜Š\"}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3d5429a5",
      "metadata": {
        "id": "3d5429a5"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "13b12c48",
      "metadata": {
        "id": "13b12c48"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c60cf0bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60cf0bf",
        "outputId": "82013290-f081-413c-889d-47d25e2929c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f1d43986",
      "metadata": {
        "id": "f1d43986"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "134965c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134965c6",
        "outputId": "b6c71397-c2e9-425d-e8be-33403912d3b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAw8r0j84fpp",
      "metadata": {
        "id": "XAw8r0j84fpp"
      },
      "source": [
        "# **ConversationBufferWindowMemory**\n",
        "\n",
        "Given that most conversations will not need context from several messages before, we also have the option of also using ConversationBufferWindowMemory. With this option we can simply control the buffer with a window of the last k messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Uon9I7BH4jIz",
      "metadata": {
        "id": "Uon9I7BH4jIz"
      },
      "outputs": [],
      "source": [
        "# ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "hVkGp84F4u7i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVkGp84F4u7i",
        "outputId": "083af957-9ae3-4450-8bc7-5902f7b36008"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-16-1653144920.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=1)\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o\",\n",
        "    temperature = 0.5\n",
        ")\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose= True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "wfQrPrHT4xSD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "wfQrPrHT4xSD",
        "outputId": "2994ff63-7b12-4710-f749-4d54de975e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Andrew\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hi Andrew! It's great to meet you. My name doesn't really have a personal touch like yours, but you can call me AI or Assistantâ€”whatever feels right to you. How's your day going so far?\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "lDTGEgXE40zU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "lDTGEgXE40zU",
        "outputId": "548e32be-1e12-4dac-b4b1-c086e39dc9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hi Andrew! It's great to meet you. My name doesn't really have a personal touch like yours, but you can call me AI or Assistantâ€”whatever feels right to you. How's your day going so far?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1+1 equals 2! It\\'s one of the simplest and most fundamental equations in mathematics. Fun fact: this concept is part of what\\'s called \"elementary arithmetic,\" which forms the basis for all kinds of advanced mathematical ideas. If you ever want to dive deeper into math, let me know!'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8HRZS7Vd43I5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "8HRZS7Vd43I5",
        "outputId": "f67e0db1-26de-46b8-c5a8-e38623e359d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2! It's one of the simplest and most fundamental equations in mathematics. Fun fact: this concept is part of what's called \"elementary arithmetic,\" which forms the basis for all kinds of advanced mathematical ideas. If you ever want to dive deeper into math, let me know!\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I donâ€™t know your name, since you havenâ€™t shared it with me yet! If youâ€™d like, you can tell me, and Iâ€™ll be happy to remember it for the rest of our conversation. What should I call you?'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "NtT6rBKg5KGk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NtT6rBKg5KGk",
        "outputId": "144d20de-3188-4d60-8445-1e8a52435441"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Human: What is my name?\\nAI: I donâ€™t know your name, since you havenâ€™t shared it with me yet! If youâ€™d like, you can tell me, and Iâ€™ll be happy to remember it for the rest of our conversation. What should I call you?'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8YmUD1855Tv",
      "metadata": {
        "id": "a8YmUD1855Tv"
      },
      "source": [
        "# **ConversationTokenBufferMemory**\n",
        "\n",
        "ConversationTokenBufferMemory simply maintains the buffer by the token count and does not maintain. There is no summarizing with this approach . In fact, it is similar to ConversationBufferWindowMemory, except instead of flushing based on the last number of messages (k), flushing is based on the number of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fqq05C9S57DY",
      "metadata": {
        "id": "fqq05C9S57DY"
      },
      "outputs": [],
      "source": [
        "# ConversationTokenBufferMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "W_dVZyWZ6CE0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_dVZyWZ6CE0",
        "outputId": "635fcda5-855f-4c86-87b7-eb28e6ef42d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-22-3816489103.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o\",  # Use deployment_name for Azure\n",
        "    model_name=\"gpt-4o\",       # Required for token counting logic\n",
        "\n",
        ")\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "lPPKMyex6PAw",
      "metadata": {
        "id": "lPPKMyex6PAw"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "memory.save_context({\"input\": \"ML is what?\"},\n",
        "                    {\"output\": \"MachineLearning!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "y-9k_v-G6Pgo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-9k_v-G6Pgo",
        "outputId": "029242a1-9a71-4cc1-defa-b7b0edc1f677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!\\nHuman: ML is what?\\nAI: MachineLearning!'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qldj1uay6Vb6",
      "metadata": {
        "id": "qldj1uay6Vb6"
      },
      "source": [
        "# **ConversationSummaryMemory**\n",
        "\n",
        "ConversationSummaryMemory does not keep the entire history in memory like ConversationBufferMemory. Nor does it maintain a window. Rather, the ConversationSummaryMemory continually summarizes the conversation as itâ€™s happening, and maintains it so we have context from the beginning of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a9oRvxZ16RWR",
      "metadata": {
        "id": "a9oRvxZ16RWR"
      },
      "outputs": [],
      "source": [
        "# ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "38EeuCte6eaj",
      "metadata": {
        "id": "38EeuCte6eaj"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o\",\n",
        "    model_name=\"gpt-4o\",\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "GUYsy_Ar6bLy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUYsy_Ar6bLy",
        "outputId": "a855a7f3-2ae5-4274-8840-9435c2aea702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-27-1139844937.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        }
      ],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9rJ8eOVs6lYF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rJ8eOVs6lYF",
        "outputId": "81c84961-81aa-463a-96d7-f3836ee31215"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and they exchange casual pleasantries. The human mentions they are just hanging out and then asks the AI about the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "sjF2YOyY6pJE",
      "metadata": {
        "id": "sjF2YOyY6pJE"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "qWPpJ1s76uyj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "qWPpJ1s76uyj",
        "outputId": "ebf5806b-45ad-4a3f-aec2-c5a8151f7499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI, and they exchange casual pleasantries. The human mentions they are just hanging out and then asks the AI about the schedule for the day.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A great demo to showcase during your lunch meeting would depend on your audience\\'s familiarity with AI and their interests. Since they\\'re driving over an hour to learn about the latest in AI, theyâ€™re probably eager to see something cutting-edge yet accessible. Here are some ideas tailored for that scenario:\\n\\n1. **Custom Question-Answering with Documents (LangChain + OpenAI)**: \\n   - If youâ€™ve had time to fine-tune something with LangChain, consider showing a live demo of a custom AI that uses a document or dataset related to their industry. For example, if they\\'re from healthcare, demonstrate the AI helping to answer queries from abstracts or research papers. This is always a hit because it shows how AI can boost productivity in specialized fields.\\n\\n2. **Conversational Agent with Plugins**:\\n   - Showcase a chatbot that employs plugins or tools, like retrieving real-time data, summarizing recent news articles, or performing small calculations seamlessly within a conversationâ€”just like OpenAIâ€™s GPT-4 with its plugin features. Let them ask it some questions themselves.\\n\\n3. **Dynamic Workflow Automation**:\\n   - Display how AI could automate a tedious, industry-specific workflow using LangChain to orchestrate different steps. For instance, an AI that processes customer support tickets, categorizes them, drafts a response, and suggests next steps.\\n\\n4. **Code Generation or Debugging Support**:\\n   - If the client is technical or working in software development, demonstrate AI writing code snippets in real-time, debugging an error, or even designing microservices. Tools like GitHub Copilot can also be part of your demo here.\\n\\n5. **Creative Use Case (Story/Blog Writing)**:\\n   - If they enjoy creative or consumer-facing applications, generate a short story with a twist or an eye-catching blog post idea based on their niche, showcasing how AI can aid content creation. Use something visual like DALLÂ·E to create relevant images, if available.\\n\\n6. **Multimodal Demo**:\\n   - If you\\'re working with GPT-4 Vision (if supported by your tools), demo how it can interpret images and generate contextually aware responses based on visual input. For example, you could upload a complex chart or a photo of something and ask the AI to analyze or explain it.\\n\\nFor any of these, simplicity and industry relevance are key. Be prepared to explain the \"how and why\" brieflyâ€”clients often love understanding whatâ€™s going on behind the scenes. Bring the focus to practical applications they can relate to or envision for their own work.\\n\\nLastly, make sure the demo runs smoothlyâ€”test it out ahead of time to avoid any hiccups during the meeting!'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7veqWQbS609N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7veqWQbS609N",
        "outputId": "ed162d21-b3cc-469c-f141-6a402a410484"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI, and they exchange casual pleasantries. The human mentions they are just hanging out and then asks the AI about the schedule for the day. The AI outlines the day's schedule, including an 8 a.m. meeting with the product team, time to work on the LangChain project, and a lunch meeting at an Italian restaurant with a customer interested in the latest in AI. The human asks for demo suggestions for the lunch meeting, and the AI provides several tailored ideas, including custom question-answering with LangChain, a conversational agent with plugins, workflow automation, code generation or debugging support, creative applications like story writing, and multimodal demonstrations such as GPT-4 Vision analysis. The AI emphasizes the importance of relevance, simplicity, and preparation for a smooth presentation.\"}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bzfsH0aCDabD",
      "metadata": {
        "id": "bzfsH0aCDabD"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Explore different memory strategies in LangChain for managing conversational context and improving interaction quality.\n",
        "\n",
        "## **Scenario**\n",
        "\n",
        "You are developing a conversational AI system that interacts with users on various topics, retaining context across multiple messages. Your goal is to implement and compare different memory strategies offered by LangChain to maintain conversation history and enhance user engagement.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* Choose Memory Strategy\n",
        "\n",
        "  * ConversationBufferMemory\n",
        "  * ConversationBufferWindowMemory\n",
        "  * ConversationTokenBufferMemory\n",
        "  * ConversationSummaryMemory\n",
        "\n",
        "* Implement Conversation Chain\n",
        "* Interact with the System\n",
        "* Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z4RY4suoDaz5",
      "metadata": {
        "id": "Z4RY4suoDaz5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
